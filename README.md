# YOHO (You Only Hear Once) üëÇ‚òùÔ∏è

![5](https://user-images.githubusercontent.com/46050829/199539339-d36849b2-f8fa-43e0-b844-350d5ed0ddd4.png)

# Introduction

YOHO does object detection in street view and provides audible scene descriptions. With YOHO, a visually impared person would be able to take out their smartphone, put in their earbuds, and take a walk in the street while their phone lets them know about their surroundings.

YOHO was created via transfer learning and fine-tuning on a YOLOv2 architechture (pre-trained on COCO dataset) on the <a href="https://bdd100k.com" target="_blank">Berkeley Deep Drive dataset</a>, and further, placing an inference engine - **<a href="https://github.com/rezmansouri/tell_a_vision" target="_blank">Tell a Vision</a>** - on top of the model to get audible output from its predictions.

This is the architecture of the system:

<p align="center">
  <img src="https://user-images.githubusercontent.com/46050829/199539577-b5ba64c5-8b9b-4f18-9f1e-358a0e26db40.png" width="70%"/>
</p>

# Links
- <a href="https://colab.research.google.com/drive/17UgFRzk617AKKuevlS-j_W1W916ZqIGe?usp=share_link" target="_blank">YOHO on Google Colab</a>
- <a href="https://www.researchgate.net/publication/365000421_YOHO_You_Only_Hear_Once_object_detection_in_street_view_with_audible_output" target="_blank">YOHO's report on ResearchGate</a>

You may also download the notebook and the report from this repository.

# Contact
If there are any questions or recommendations, you can reach out to me at <a href="mailto:itsrezamansouri@gmail.com" target="_blank">itsrezamansouri@gmail.com</a>.
